{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea203afe-a5c2-4087-8f31-179ffe5b4ad1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'user_item_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 40\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# === Set K and calculate all metrics ===\u001b[39;00m\n\u001b[0;32m     39\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m---> 40\u001b[0m precision \u001b[38;5;241m=\u001b[39m precision_at_k(\u001b[43muser_item_df\u001b[49m, k)\n\u001b[0;32m     41\u001b[0m recall \u001b[38;5;241m=\u001b[39m recall_at_k(user_item_df, k)\n\u001b[0;32m     42\u001b[0m mrr \u001b[38;5;241m=\u001b[39m mrr_at_k(user_item_df, k)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'user_item_df' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def precision_at_k(dataframe, k):\n",
    "    hits = 0\n",
    "    for _, row in dataframe.iterrows():\n",
    "        if row['true_item'] in row['predicted_items'][:k]:\n",
    "            hits += 1\n",
    "    return hits / (len(dataframe) * k)\n",
    "\n",
    "def recall_at_k(dataframe, k):\n",
    "    # Since there's only one true item, it's same as hit ratio\n",
    "    return hit_ratio_at_k(dataframe, k)\n",
    "\n",
    "def mrr_at_k(dataframe, k):\n",
    "    rr_total = 0\n",
    "    for _, row in dataframe.iterrows():\n",
    "        try:\n",
    "            rank = row['predicted_items'][:k].index(row['true_item']) + 1\n",
    "            rr_total += 1 / rank\n",
    "        except ValueError:\n",
    "            rr_total += 0\n",
    "    return rr_total / len(dataframe)\n",
    "\n",
    "def ndcg_at_k(dataframe, k):\n",
    "    def dcg(rel_list):\n",
    "        return sum([rel / np.log2(idx + 2) for idx, rel in enumerate(rel_list)])\n",
    "    \n",
    "    total_ndcg = 0\n",
    "    for _, row in dataframe.iterrows():\n",
    "        rel = [1 if item == row['true_item'] else 0 for item in row['predicted_items'][:k]]\n",
    "        ideal_rel = sorted(rel, reverse=True)\n",
    "        dcg_score = dcg(rel)\n",
    "        idcg_score = dcg(ideal_rel)\n",
    "        ndcg = dcg_score / idcg_score if idcg_score != 0 else 0\n",
    "        total_ndcg += ndcg\n",
    "    return total_ndcg / len(dataframe)\n",
    "\n",
    "# === Set K and calculate all metrics ===\n",
    "k = 5\n",
    "precision = precision_at_k(user_item_df, k)\n",
    "recall = recall_at_k(user_item_df, k)\n",
    "mrr = mrr_at_k(user_item_df, k)\n",
    "ndcg = ndcg_at_k(user_item_df, k)\n",
    "\n",
    "# === Print results ===\n",
    "print(f\"\\nüìä Evaluation Metrics @ {k}:\")\n",
    "print(f\"Precision@{k}: {precision:.4f}\")\n",
    "print(f\"Recall@{k}:    {recall:.4f}\")\n",
    "print(f\"MRR@{k}:       {mrr:.4f}\")\n",
    "print(f\"nDCG@{k}:      {ndcg:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3615bd7-ef5c-43b1-97d9-35022ff3f1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå Available columns:\n",
      "['User ID', 'Item ID', 'h_ij', 'd_ij', 'avg_score', 'review_rank', 'n_prime', 'z_ij', 'most_ij', 'sum_z', 'q_ij', 'sum_q', 'top_ij']\n",
      "\n",
      "üîç Sample data:\n",
      "          User ID     Item ID      h_ij      d_ij  avg_score  review_rank  \\\n",
      "0   AO94DHGC771SJ  0528881469  0.000000  0.653309   0.156307            1   \n",
      "1   AMO214LNFCEI4  0528881469  0.163296  0.202288   0.211416            2   \n",
      "2  A3N7T0DY83Y4IG  0528881469  0.698923  0.089906   0.451825            3   \n",
      "3  A1H8PY3QHMQQA0  0528881469  0.137781  0.040832   0.141980            4   \n",
      "4  A24EV6RXELQZ63  0528881469  0.000000  0.013665   0.038471            5   \n",
      "\n",
      "   n_prime      z_ij   most_ij     sum_z      q_ij     sum_q    top_ij  \n",
      "0        5  1.000000  0.683242  1.463611  4.000000  6.416667  0.623377  \n",
      "1        5  0.250000  0.170810  1.463611  1.500000  6.416667  0.233766  \n",
      "2        5  0.111111  0.075916  1.463611  0.666667  6.416667  0.103896  \n",
      "3        5  0.062500  0.042703  1.463611  0.250000  6.416667  0.038961  \n",
      "4        5  0.040000  0.027330  1.463611  0.000000  6.416667  0.000000  \n",
      "\n",
      "üìä Evaluation Metrics @ 5:\n",
      "Hit Ratio@5:  1.0000\n",
      "Precision@5:  0.2000\n",
      "Recall@5:     1.0000\n",
      "MRR@5:         1.0000\n",
      "nDCG@5:        1.0000\n",
      "\n",
      "‚úÖ File saved as 'user_predictions_evaluation.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# === Step 1: Load the CSV ===\n",
    "file_path = \"reliability_scores_matrics.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# === Step 2: Show the structure of the data ===\n",
    "print(\"\\nüìå Available columns:\")\n",
    "print(df.columns.tolist())\n",
    "print(\"\\nüîç Sample data:\")\n",
    "print(df.head())\n",
    "\n",
    "# === Step 3: Define columns ===\n",
    "# Replace with actual column names from your dataset\n",
    "GROUND_TRUTH_COL = 'Item ID'       # True item per user\n",
    "PREDICTED_COL = 'top_ij'           # Assumed as predicted score for ranking\n",
    "\n",
    "# === Step 4: Generate Top-K Predictions Per User ===\n",
    "df_sorted = df.sort_values(by=['User ID', PREDICTED_COL], ascending=[True, False])\n",
    "top_k_df = df_sorted.groupby('User ID').head(5)\n",
    "\n",
    "# === Step 5: Create predicted list per user ===\n",
    "user_predicted = top_k_df.groupby('User ID')['Item ID'].apply(list).reset_index()\n",
    "user_predicted.columns = ['User ID', 'predicted_items']\n",
    "\n",
    "# === Step 6: Get true item per user (assuming highest 'top_ij' is ground truth) ===\n",
    "user_true = df_sorted.groupby('User ID').first().reset_index()\n",
    "user_true = user_true[['User ID', 'Item ID']]\n",
    "user_true.columns = ['User ID', 'true_item']\n",
    "\n",
    "# === Step 7: Merge both ===\n",
    "df = pd.merge(user_true, user_predicted, on='User ID')\n",
    "\n",
    "# === Step 8: Define evaluation metrics ===\n",
    "def hit_ratio_at_k(dataframe, k):\n",
    "    hits = sum(row['true_item'] in row['predicted_items'][:k] for _, row in dataframe.iterrows())\n",
    "    return hits / len(dataframe)\n",
    "\n",
    "def precision_at_k(dataframe, k):\n",
    "    hits = sum(row['true_item'] in row['predicted_items'][:k] for _, row in dataframe.iterrows())\n",
    "    return hits / (len(dataframe) * k)\n",
    "\n",
    "def recall_at_k(dataframe, k):\n",
    "    # 1 relevant item per user\n",
    "    return hit_ratio_at_k(dataframe, k)\n",
    "\n",
    "def mrr_at_k(dataframe, k):\n",
    "    rr_total = 0\n",
    "    for _, row in dataframe.iterrows():\n",
    "        try:\n",
    "            rank = row['predicted_items'][:k].index(row['true_item']) + 1\n",
    "            rr_total += 1 / rank\n",
    "        except ValueError:\n",
    "            rr_total += 0\n",
    "    return rr_total / len(dataframe)\n",
    "\n",
    "def ndcg_at_k(dataframe, k):\n",
    "    def dcg(rel_list):\n",
    "        return sum([rel / np.log2(idx + 2) for idx, rel in enumerate(rel_list)])\n",
    "    \n",
    "    total_ndcg = 0\n",
    "    for _, row in dataframe.iterrows():\n",
    "        rel = [1 if item == row['true_item'] else 0 for item in row['predicted_items'][:k]]\n",
    "        ideal_rel = sorted(rel, reverse=True)\n",
    "        dcg_score = dcg(rel)\n",
    "        idcg_score = dcg(ideal_rel)\n",
    "        total_ndcg += dcg_score / idcg_score if idcg_score != 0 else 0\n",
    "    return total_ndcg / len(dataframe)\n",
    "\n",
    "# === Step 9: Evaluate ===\n",
    "k = 5\n",
    "print(f\"\\nüìä Evaluation Metrics @ {k}:\")\n",
    "print(f\"Hit Ratio@{k}:  {hit_ratio_at_k(df, k):.4f}\")\n",
    "print(f\"Precision@{k}:  {precision_at_k(df, k):.4f}\")\n",
    "print(f\"Recall@{k}:     {recall_at_k(df, k):.4f}\")\n",
    "print(f\"MRR@{k}:         {mrr_at_k(df, k):.4f}\")\n",
    "print(f\"nDCG@{k}:        {ndcg_at_k(df, k):.4f}\")\n",
    "\n",
    "# === Step 10: Save to CSV ===\n",
    "df.to_csv(\"user_predictions_evaluation.csv\", index=False)\n",
    "print(\"\\n‚úÖ File saved as 'user_predictions_evaluation.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bff520b-c941-4436-88ad-f293b21fed2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
